ACA

Branch prediction
       dynamic
       static
       correlating

ILP
	- VLIW
	- Static scheduling
	  Compiler rearranges instructions to avoid stalls
	       - Code motion
	       - Loop unrolling & peeling
	       - Sw pipeline
	       - Global code scheduling (across basic blocks
	       	      - Trace scheduling
		      - Superblock scheduling
		      - Hyperblock scheduling
		      - Speculative trace scheduling
	- Superscalar
	- Dynamic scheduling
	  Hw rearranges instruction executions at runtime
	  In order Fetch, Issue
	  Out of order execution -> WAR, WAW
	  Split ID into:
	  	- Issue: decode instructions and check for structural hazards; in-order issue
		- Read: wait until no data hazards, then read operands; out-of-order read
	  Methods:
	      - Register renaming, rename all destinatin registers, including those with a pending read or write for anearlier instruction
	      - Scoreboarding (Appendix A)
	      	No forwarding, no register renaming
		Multiple access to register files, we can read the same content of a register into different instructions
	      	Hazard detection
		       - Issue: Struct, WAW. If present, stalls. N.B. In-order issue!!
		       - Read operands: RAW
		       - Exec: N.B. out of order!!
		       - Write: WAR

	      - Tomasulo
	      	Key idea is CDB and RSs. When RS needs a value, if not ready, it stores the FUs that will provide it. Then, when the FU finishes, it will directly provide the result to the RSs waiting for it, even skipping the RF if it is no more necessary
		Generally, not possible to have two writes in the same CC, but possible to have multiple completion in same CC
	        Hazard detection
		       - Issue: if there are structural hazards -> stall
		       	        WAR and WAW eliminated by register renaming (reservation stations)
		       - Exec: RAW avoided by executing an instruction only when its operands are available (if not ready watch CDB
		       - Write: write on CDB and from here to RF and RSs waiting for this result; stores also writes data to memory in this stage; mark RS available
Parallel processing

Exceptions
	

Cache
	AMAT= HitTime + MissRate * MissPenalty
	Block is the unit of transfer b/w cache and memory
	Causes of cache misses:
	       - Compulsory (cold start)
	       	 first reference to a line
	       - Capacity
	       	 cache is too small to hold all data needed by programs
	       - Conflict
	       	 Misses due to block placement policies (i.e. associativity)

	Questions:
		- Where to place block?
		  Fully associative (block placed anywhere, like set associative with just 1 set)
		  Set associative (f.e. if memory address=12 and we have 4 sets of 2 blocks each (2-way set associative) we can place the block in every block of the set 12mod4=0)
		  Direct mapped (... we have 8 blocks, we can place the memory block only into block 12mod8=4; like 1-way set associative with 8 sets)
		- How to find a block?
		  Each cache block includes a tag to see which memory address it corresponds to
		- Which block to replace on a miss
		  Random (the larger the cache & associativity, the small is difference b/w random and LRU)
		  Last Recently Used
		  FIFO (aka Round Robin)
		- What happens on a write?
		  Cache hit:
		  	- Write through
			  write both cache and underlying cache/memory (increase bandwidth, but simplifies cache coherency and debug)
			  often there is a buffer that holds data awaiting to write to memory
			  We can always discard cached data bcz the most up-to-date data is in underlying cache/memory
			  We only have to add a valid bit to the cache to know if the block contains a valid address (and hence must be checked when searching a block)
			- Write back
			  write only cache, memory is updated when block is evicted
			  We can't just discard cached data bcz it may have not been written yet to underlying cache/memory
			  We have a valid bit and a dirty bit to indicate that the block has been modified, but not yet written to underlying c./m.
	         Cache miss:
		       - No write allocate
		       	 only write to main memory
		       - Write allocate
		       	 fetch missing block into cache
		       - Common combinations
		       	 write through and no write allocate
			 write back and write allocate
	How to reduce misses:

	Advanced techniques to reduce misses:
		- Reduce miss rate through:
		  	- Larger block size
			  Take advantage of spatial locality by increasing number of words inside each block
	       	 	  Reduces capacity and compulsory misses
		 	  Increases conflict misses and miss penalty			  
		  	- Higher associativity  (through fully associative)
               	       	  Reduces conflict misses
			  Increases hit time and power consumption
	     	        - Larger cache sizes
	     	       	  Reduces capacity and conflict misses
			  Increases hit time and power consumption
		  	- Victim cache
			  Add buffer to place data discarded from cache when I have to do a block replacement
		  	- Pseudo-associativity
			  Divide cache into 2 parts and assume cache is only on the 1st part; on a miss, I go to the 2nd part (pseudo-hit time)
			  Drawback: cpu pipeline is hard if hit-time is different, so used for caches not directly tied to processor (e.g. L2 cache)
		  	- HW prefetching Instr, data
			  On a miss, I bring to the cache not only the requested blocks, but also adjacent blocks.
			  1st block is directly placed into cache, 2nd block on a stream buffer
			  Relies on utilizing memory bandwidth that otherwise would be unused
		  	- SW prefetching data
			  Make sense only if processor can continue while prefetching the data; so, we mostly use non-blocking cachesz
			  Two flavors
			      - Register prefetch: Load data into registers
			      - Cache prefetch: Load data into cache
		  	- Compiler optimization
			  Reorder procedures in memory so as to reduce conflict misses
			  	  - Merging arrays
				  - Loop interchange
				  - Loop fusion
				  - Blocking
				  
		- Reduce miss penalty through:
		  	- Read priority over write on miss
			  When we use a write-through with a write buffer, we can have RAW conflicts w/ main memory reads if the updated value is in the buffer but not yet in main memory. So, before going to memory, we can check this buffer on a read miss.
			  W/ write-back, normally on a read miss we write the dirty block to memory and then do the read. Instead, we can copy the dirty block to a write buffer, then do the read, then do the write
		  	- Subblock placement
			  Don't load full blocks on a miss. We have to have valid bits per subblock
		  	- Early restart anc critical instruction first
			  Don't wait for full block to be loaded to restart the cpu
			  	- Early restart
				  a.s.a. the requested word arrives, send it to the CPU and let it continue the execution
				- Critical word first
				  Request the missed word first from memory and send it to the CPU to continue the execution.
				  Widely used bcz long blocks are more popular today
		  	- Non blocking caches (Hit under miss, miss under miss)
			  If we have out-of-order completion, this optimization allow the cache to supply cache hits during a miss instead of ignoring the request of the processor (hit under miss).
			  A further improvement is to overlap multiple misses and is useful only if the memory can service multiple misses
		  	- Multi-level cache (L2, L3)
			  L1 cache small and fast to cope w/ fast clock cycle, L2 cache large to overcome the gap b/w processor memory and main memory.
			  L2 cache have high size, larger blocks, higher associativity: the focus is on avoid misses
			  Generally, L1 write-back, L2 write-trough
			  Inclusion policy
			  	    - Inclusive: inner cache (L1) can only hold lines also present in outer cache (L2)
				    - Exclusive: ...
				    
		- Reduce hit time through:
			- Subblock placement (see before)
			- Small & simple caches -> drawback: Increase miss rate
			  Index tag memory and then compare takes time
			  Small size takes less time to index.
			  Direct mapping can overlap tag check w/ data transmission bcz we have no choice on a block placement
			- Avoiding address translation
			  Caches must cope w/ translation of virtual address from processor to physical address to access memory
			- Pipelining writes
			- Way prediction
			- Trace cache
			
		- Increase bandwidth
		  	- Pipelined cache access
			  In this way the effective latency of an L1 cache can be multiple CCs. Drawback: we have slow hits, increased number of pipeline stages and so greater penaltis on branch misprediction
			- Multiple banks
			  Rather than treat cache as one block, divide it into independent banks that can support simultaneous access

**** General notes ****
Stalls in static pipeline 
	Stalls the pipeline, if there is a data dependence, starting from the instruction using the result.
	In simple pipeline, we don't execute ID phase until RAW conflicts are resolved
	When instruction stalls, no further instructions can enter IF stage

Dynamic pipeline when we have IF ID IS (Exec) WB
	We read the operands in IS stage. We can't issue if we don't have all the operands available.
	N.B.: W happens in 1st half of CC, read (so IS) in 2nd half. So, often we can do IS in the same CC of WB when there is a RAW
	If previous instruction is stalling in IF (ID) phase, further instructions have to stall n.m.w. When the stall ends and we enter ID (IS) phase, in the same CC the following instruction can start to IF (ID)

**** Chapters ****
2
A (7)
3
4
5

	 
