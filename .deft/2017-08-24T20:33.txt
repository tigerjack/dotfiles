ACA

Branch
	- Execution
	  B. outcome and B. target address are ready at the end of EX stage
	  Conditional b. are solven when PC is updated at the end of MEM stage
	  Solution in MIPS: compare registers, computes b. target address and update PC during ID stage -> we need (generally, but not always) 1 CC stall after each branch OR a flush of only one instruction following the branch
	- Hazards
		
	- Prediction
       	       - Static
			- Always not taken
			  1 CC penalty for misprediction
			- Always taken
			  Makes sense for pipelines where branch target is known before branch outcome (not for MIPS)
			- Backward taken, forward not taken
			- profile-driven prediction
			  Based on previous runs and compiler hints
			- Delayed branch
			  Compiler add an independent instruction in the branch delay slot (for MIPS 1 delay slot)
			  Branch delay slot can be scheduled:
			  	 - From before
				   i.e. an independent instruction from before the branch
				 - From target
				   i.e. an ind. instr. taken from the target of the branch; preferred when branch is taken w/ high probability (f.e. backward branches)
				 - From fall-through
				   i.e. .. from the non-taken path; prefered when branch is not taken w/ high probability (f.e. forward branches)
				 The last two are generally used when the first one (from before) can't be used (f.e. bcz the branch condition depends on the previous instruction)
			  Main limitations:
			       - Restrictions on instructions that can be scheduled into delay slot
			       - Ability of compiler to statically predict outcome of a branch
			       - Deep pipelines have more than 1 delay slots
			       
	       - Dynamic
	       	 The IF unit uses two modules
		     - Branch Outcome Predictor to predict taken/not taken
		       We have a Branch History Table that contains a bit for each entry that sas whether the branch was recently taken or not
		       To increase accuracy, we can use a 2-bit BHT: the prediction must miss twice before it's changed
		       n-bit BHT, w/ n>2, doesn't perform so well
		       The prediction bit(s) could has been put there by another branch w/ th same low-order address bits; who cares, it's a prediction. To avoid this problem we can increase the number of rows in BHT or use a hashing function
		       Indexed by the lower portion of the address of branch instruction (as for BTP)
		     - Branch Target Predictor to predict target address in case of taken
		       BTP (or Branch Target Buffer) is a cache which has different entries
		       	   - Address of the branch (usually the lowest bits of PC)
			   - Predicted target address (only for taken branch, expressed as PC relative)
			   - Tag to say if the branch has been taken previously)
		 Then, if b. is predicted not taken, PC+4; if it is predicted taken, BTP gives target address
		       
       	       - Correlating
	       	 We guess that the behavior of recent branches is correlated, i.e. the prediction has to be made not only basing on the behavior of current branch, but also of other branches
		 (1, 1) Correlating Predictors means a 1-bit predictor w/ 1 bit of correlaton: behavior of last branch is used to choose among a pair of 1-bit branch predictors
		 (m, n) uses the behavior of the last m branches to choose from 2^m branch predictor, each of which is an n-bit predictor for a single branch
		 (0, 2) is a 2-bit predictor w/ no global history
		 To obtain this result, the global history of the most recent m branches is recorded in an m-bit shift register, where each bit records whether the branch was taken or not taken. The BP buffer can then be indexed using a concatenation of the low-order bits from the branch address (as for the simple scheme before) and the m bits of the shift register.
		 F.e. in a (2, 2) correlating predictor where we use the 4 lower bits of the branch address we have an index of 4+2=6 bits, leading to 2^6=64 counters

ILP
	- VLIW
	- Static scheduling
	  Compiler rearranges instructions to avoid stalls
	       - Code motion
	       - Loop unrolling & peeling
	       - Sw pipeline
	       - Global code scheduling (across basic blocks
	       	      - Trace scheduling
		      - Superblock scheduling
		      - Hyperblock scheduling
		      - Speculative trace scheduling
	- Superscalar
	- Dynamic scheduling
	  Hw rearranges instruction executions at runtime
	  In order Fetch, Issue
	  Out of order execution -> WAR, WAW
	  Split ID into:
	  	- Issue: decode instructions and check for structural hazards; in-order issue
		- Read: wait until no data hazards, then read operands; out-of-order read
	  Methods:
	      - Register renaming, rename all destinatin registers, including those with a pending read or write for anearlier instruction
	      - Scoreboarding (Appendix A)
	      	No forwarding, no register renaming
		Multiple access to register files, we can read the same content of a register into different instructions
	      	Hazard detection
		       - Issue: Struct, WAW. If present, stalls. N.B. In-order issue!!
		       - Read operands: RAW
		       - Exec: N.B. out of order!!
		       - Write: WAR

	      - Tomasulo
	      	Key idea is CDB and RSs. When RS needs a value, if not ready, it stores the FUs that will provide it. Then, when the FU finishes, it will directly provide the result to the RSs waiting for it, even skipping the RF if it is no more necessary
		Generally, not possible to have two writes in the same CC, but possible to have multiple completion in same CC
	        Hazard detection
		       - Issue: if there are structural hazards -> stall
		       	        WAR and WAW eliminated by register renaming (reservation stations)
		       - Exec: RAW avoided by executing an instruction only when its operands are available (if not ready watch CDB
		       - Write: write on CDB and from here to RF and RSs waiting for this result; stores also writes data to memory in this stage; mark RS available
Parallel processing

Exceptions
	
	

Cache
	AMAT= HitTime + MissRate * MissPenalty
	Block is the unit of transfer b/w cache and memory
	Causes of cache misses:
	       - Compulsory (cold start)
	       	 first reference to a line
	       - Capacity
	       	 cache is too small to hold all data needed by programs
	       - Conflict
	       	 Misses due to block placement policies (i.e. associativity)

	Questions:
		- Where to place block?
		  Fully associative (block placed anywhere, like set associative with just 1 set)
		  Set associative (f.e. if memory address=12 and we have 4 sets of 2 blocks each (2-way set associative) we can place the block in every block of the set 12mod4=0)
		  Direct mapped (... we have 8 blocks, we can place the memory block only into block 12mod8=4; like 1-way set associative with 8 sets)
		- How to find a block?
		  Each cache block includes a tag to see which memory address it corresponds to
		- Which block to replace on a miss
		  Random (the larger the cache & associativity, the small is difference b/w random and LRU)
		  Last Recently Used
		  FIFO (aka Round Robin)
		- What happens on a write?
		  Cache hit:
		  	- Write through
			  write both cache and underlying cache/memory (increase bandwidth, but simplifies cache coherency and debug)
			  often there is a buffer that holds data awaiting to write to memory
			  We can always discard cached data bcz the most up-to-date data is in underlying cache/memory
			  We only have to add a valid bit to the cache to know if the block contains a valid address (and hence must be checked when searching a block)
			- Write back
			  write only cache, memory is updated when block is evicted
			  We can't just discard cached data bcz it may have not been written yet to underlying cache/memory
			  We have a valid bit and a dirty bit to indicate that the block has been modified, but not yet written to underlying c./m.
	         Cache miss:
		       - No write allocate
		       	 only write to main memory
		       - Write allocate
		       	 fetch missing block into cache
		       - Common combinations
		       	 write through and no write allocate
			 write back and write allocate
	How to reduce misses:

	Advanced techniques to reduce misses:
		- Reduce miss rate through:
		  	- Larger block size
			  Take advantage of spatial locality by increasing number of words inside each block
	       	 	  Reduces capacity and compulsory misses
		 	  Increases conflict misses and miss penalty			  
		  	- Higher associativity  (through fully associative)
               	       	  Reduces conflict misses
			  Increases hit time and power consumption
	     	        - Larger cache sizes
	     	       	  Reduces capacity and conflict misses
			  Increases hit time and power consumption
		  	- Victim cache
			  Add buffer to place data discarded from cache when I have to do a block replacement
		  	- Pseudo-associativity
			  Divide cache into 2 parts and assume cache is only on the 1st part; on a miss, I go to the 2nd part (pseudo-hit time)
			  Drawback: cpu pipeline is hard if hit-time is different, so used for caches not directly tied to processor (e.g. L2 cache)
		  	- HW prefetching Instr, data
			  On a miss, I bring to the cache not only the requested blocks, but also adjacent blocks.
			  1st block is directly placed into cache, 2nd block on a stream buffer
			  Relies on utilizing memory bandwidth that otherwise would be unused
		  	- SW prefetching data
			  Make sense only if processor can continue while prefetching the data; so, we mostly use non-blocking cachesz
			  Two flavors
			      - Register prefetch: Load data into registers
			      - Cache prefetch: Load data into cache
		  	- Compiler optimization
			  Reorder procedures in memory so as to reduce conflict misses
			  	  - Merging arrays
				  - Loop interchange
				  - Loop fusion
				  - Blocking
				  
		- Reduce miss penalty through:
		  	- Read priority over write on miss
			  When we use a write-through with a write buffer, we can have RAW conflicts w/ main memory reads if the updated value is in the buffer but not yet in main memory. So, before going to memory, we can check this buffer on a read miss.
			  W/ write-back, normally on a read miss we write the dirty block to memory and then do the read. Instead, we can copy the dirty block to a write buffer, then do the read, then do the write
		  	- Subblock placement
			  Don't load full blocks on a miss. We have to have valid bits per subblock
		  	- Early restart anc critical instruction first
			  Don't wait for full block to be loaded to restart the cpu
			  	- Early restart
				  a.s.a. the requested word arrives, send it to the CPU and let it continue the execution
				- Critical word first
				  Request the missed word first from memory and send it to the CPU to continue the execution.
				  Widely used bcz long blocks are more popular today
		  	- Non blocking caches (Hit under miss, miss under miss)
			  If we have out-of-order completion, this optimization allow the cache to supply cache hits during a miss instead of ignoring the request of the processor (hit under miss).
			  A further improvement is to overlap multiple misses and is useful only if the memory can service multiple misses
		  	- Multi-level cache (L2, L3)
			  L1 cache small and fast to cope w/ fast clock cycle, L2 cache large to overcome the gap b/w processor memory and main memory.
			  L2 cache have high size, larger blocks, higher associativity: the focus is on avoid misses
			  Generally, L1 write-back, L2 write-trough
			  Inclusion policy
			  	    - Inclusive: inner cache (L1) can only hold lines also present in outer cache (L2)
				    - Exclusive: ...
				    
		- Reduce hit time through:
			- Subblock placement (see before)
			- Small & simple caches -> drawback: Increase miss rate
			  Index tag memory and then compare takes time
			  Small size takes less time to index.
			  Direct mapping can overlap tag check w/ data transmission bcz we have no choice on a block placement
			- Avoiding address translation
			  Caches must cope w/ translation of virtual address from processor to physical address to access memory
			- Pipelining writes
			- Way prediction
			- Trace cache
			
		- Increase bandwidth
		  	- Pipelined cache access
			  In this way the effective latency of an L1 cache can be multiple CCs. Drawback: we have slow hits, increased number of pipeline stages and so greater penaltis on branch misprediction
			- Multiple banks
			  Rather than treat cache as one block, divide it into independent banks that can support simultaneous access

**** General notes ****
Stalls in static pipeline 
	Stalls the pipeline, if there is a data dependence, starting from the instruction using the result.
	In simple pipeline, we don't execute ID phase until RAW conflicts are resolved
	When instruction stalls, no further instructions can enter IF stage

Dynamic pipeline when we have IF ID IS (Exec) WB
	We read the operands in IS stage. We can't issue if we don't have all the operands available.
	N.B.: W happens in 1st half of CC, read (so IS) in 2nd half. So, often we can do IS in the same CC of WB when there is a RAW
	If previous instruction is stalling in IF (ID) phase, further instructions have to stall n.m.w. When the stall ends and we enter ID (IS) phase, in the same CC the following instruction can start to IF (ID)
	When we say "check RAW hazard in IS phase" it means that we can't start IS until all RAW hazards are cleared


**** Chapters ****
2
A (7)
3
4
5

	 
